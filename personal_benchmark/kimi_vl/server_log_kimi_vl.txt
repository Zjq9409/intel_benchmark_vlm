INFO 06-02 20:16:23 [__init__.py:243] Automatically detected platform cuda.
INFO 06-02 20:17:00 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-02 20:17:00 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-02 20:17:00 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-02 20:17:01 [api_server.py:1289] vLLM API server version 0.9.0
INFO 06-02 20:17:01 [cli_args.py:300] non-default args: {'host': '127.0.0.1', 'port': 8008, 'chat_template': './template_chatml.jinja', 'model': '/weights/Kimi-VL-A3B-Thinking', 'tokenizer': '/weights/Kimi-VL-A3B-Thinking', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 16384, 'distributed_executor_backend': 'ray', 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 64}
INFO 06-02 20:17:08 [config.py:793] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 06-02 20:17:08 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 06-02 20:17:08 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 06-02 20:17:11 [__init__.py:243] Automatically detected platform cuda.
INFO 06-02 20:17:14 [core.py:438] Waiting for init message from front-end.
INFO 06-02 20:17:14 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-02 20:17:14 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-02 20:17:14 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-02 20:17:14 [core.py:65] Initializing a V1 LLM engine (v0.9.0) with config: model='/weights/Kimi-VL-A3B-Thinking', speculative_config=None, tokenizer='/weights/Kimi-VL-A3B-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/weights/Kimi-VL-A3B-Thinking, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
2025-06-02 20:17:17,183	INFO worker.py:1888 -- Started a local Ray instance.
INFO 06-02 20:17:18 [ray_utils.py:333] No current placement group found. Creating a new placement group.
INFO 06-02 20:17:19 [ray_distributed_executor.py:176] use_ray_spmd_worker: True
[36m(pid=1668)[0m INFO 06-02 20:17:22 [__init__.py:243] Automatically detected platform cuda.
INFO 06-02 20:17:25 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
INFO 06-02 20:17:25 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USAGE_SOURCE', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']
INFO 06-02 20:17:25 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.46it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:00<00:01,  2.63it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:01,  2.64it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:01<00:00,  3.14it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:01<00:00,  2.78it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:02<00:00,  2.72it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:02<00:00,  2.69it/s]
[36m(RayWorkerWrapper pid=1661)[0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:02<00:00,  2.73it/s]
[36m(RayWorkerWrapper pid=1661)[0m 
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:25 [__init__.py:31] Available plugins for group vllm.general_plugins:
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:25 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:25 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(RayWorkerWrapper pid=1667)[0m WARNING 06-02 20:17:27 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d3ac84bdd30>
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:28 [utils.py:1077] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:28 [pynccl.py:69] vLLM is using nccl==2.26.2
[36m(pid=1661)[0m INFO 06-02 20:17:22 [__init__.py:243] Automatically detected platform cuda.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=1667)[0m WARNING 06-02 20:17:28 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[36m(RayWorkerWrapper pid=1661)[0m INFO 06-02 20:17:28 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_b290c651'), local_subscribe_addr='ipc:///tmp/086c3b0d-18ef-414e-8c67-34002c0c365b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:28 [parallel_state.py:1064] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[36m(RayWorkerWrapper pid=1667)[0m WARNING 06-02 20:17:29 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:31 [topk_topp_sampler.py:48] Using FlashInfer for top-p & top-k sampling.
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:25 [__init__.py:31] Available plugins for group vllm.general_plugins:[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:25 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:25 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:31 [gpu_model_runner.py:1531] Starting to load model /weights/Kimi-VL-A3B-Thinking...
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:31 [cuda.py:180] Using Triton MLA backend on V1 engine.
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:31 [backends.py:35] Using InductorAdaptor
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:17:34 [default_loader.py:280] Loading weights took 2.25 seconds
[36m(RayWorkerWrapper pid=1664)[0m WARNING 06-02 20:17:27 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78e646fee150>[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:28 [utils.py:1077] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:28 [pynccl.py:69] vLLM is using nccl==2.26.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m WARNING 06-02 20:17:28 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:28 [parallel_state.py:1064] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:34 [gpu_model_runner.py:1549] Model loading took 8.3433 GiB and 2.563488 seconds
[36m(RayWorkerWrapper pid=1668)[0m WARNING 06-02 20:17:29 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1661)[0m INFO 06-02 20:17:34 [gpu_model_runner.py:1863] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 2 image items of the maximum feature size.
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:44 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/a6392efc4c/rank_2_0 for vLLM's torch.compile
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:44 [backends.py:469] Dynamo bytecode transform time: 7.19 s
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:31 [topk_topp_sampler.py:48] Using FlashInfer for top-p & top-k sampling.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:31 [gpu_model_runner.py:1531] Starting to load model /weights/Kimi-VL-A3B-Thinking...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:17:31 [cuda.py:180] Using Triton MLA backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:31 [backends.py:35] Using InductorAdaptor[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1661)[0m INFO 06-02 20:17:34 [default_loader.py:280] Loading weights took 2.59 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1661)[0m INFO 06-02 20:17:34 [gpu_model_runner.py:1549] Model loading took 8.3433 GiB and 2.966343 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:17:34 [gpu_model_runner.py:1863] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 2 image items of the maximum feature size.[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:17:49 [backends.py:158] Cache the graph of shape None for later use
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:17:46 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/a6392efc4c/rank_3_0 for vLLM's torch.compile[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:17:46 [backends.py:469] Dynamo bytecode transform time: 9.60 s[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:18:23 [backends.py:170] Compiling a graph for general shape takes 39.13 s
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:17:51 [backends.py:158] Cache the graph of shape None for later use[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:18:33 [backends.py:170] Compiling a graph for general shape takes 47.50 s
[36m(RayWorkerWrapper pid=1661)[0m INFO 06-02 20:18:33 [backends.py:170] Compiling a graph for general shape takes 47.59 s
[36m(RayWorkerWrapper pid=1667)[0m WARNING 06-02 20:18:42 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_GeForce_RTX_4090_D.json
[36m(RayWorkerWrapper pid=1668)[0m INFO 06-02 20:18:35 [backends.py:170] Compiling a graph for general shape takes 48.53 s
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:18:51 [monitor.py:33] torch.compile takes 55.97 s in total
[36m(RayWorkerWrapper pid=1664)[0m WARNING 06-02 20:18:42 [fused_moe.py:682] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_GeForce_RTX_4090_D.json[32m [repeated 3x across cluster][0m
INFO 06-02 20:18:52 [kv_cache_utils.py:637] GPU KV cache size: 90,672 tokens
INFO 06-02 20:18:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 5.53x
INFO 06-02 20:18:52 [kv_cache_utils.py:637] GPU KV cache size: 90,672 tokens
INFO 06-02 20:18:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 5.53x
INFO 06-02 20:18:52 [kv_cache_utils.py:637] GPU KV cache size: 90,672 tokens
INFO 06-02 20:18:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 5.53x
INFO 06-02 20:18:52 [kv_cache_utils.py:637] GPU KV cache size: 90,672 tokens
INFO 06-02 20:18:52 [kv_cache_utils.py:640] Maximum concurrency for 16,384 tokens per request: 5.53x
[36m(RayWorkerWrapper pid=1667)[0m INFO 06-02 20:19:20 [gpu_model_runner.py:1933] Graph capturing finished in 28 secs, took 0.95 GiB
[36m(RayWorkerWrapper pid=1664)[0m INFO 06-02 20:18:51 [monitor.py:33] torch.compile takes 46.32 s in total[32m [repeated 3x across cluster][0m
INFO 06-02 20:19:20 [core.py:167] init engine (profile, create kv cache, warmup model) took 105.85 seconds
WARNING 06-02 20:19:21 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 06-02 20:19:21 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 06-02 20:19:23 [loggers.py:134] vllm cache_config_info with initialization after num_gpu_blocks is: 5667
WARNING 06-02 20:19:24 [api_server.py:1186] Using supplied chat template: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
WARNING 06-02 20:19:24 [api_server.py:1186] {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
WARNING 06-02 20:19:24 [api_server.py:1186] It is different from official chat template '/weights/Kimi-VL-A3B-Thinking'. This discrepancy may lead to performance degradation.
INFO 06-02 20:19:24 [api_server.py:1336] Starting vLLM API server on http://127.0.0.1:8008
INFO 06-02 20:19:24 [launcher.py:28] Available routes are:
INFO 06-02 20:19:24 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /health, Methods: GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /load, Methods: GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /ping, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /ping, Methods: GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /version, Methods: GET
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /classify, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /score, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-02 20:19:24 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [1062]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 06-02 20:24:28 [logger.py:42] Received request cmpl-4bc20e2c2d11435f8576c7e510db0c67-0: prompt: 'ËØ∑Âü∫‰∫éOpenCVÂÜô‰∏ÄÊÆµ‰ª£Á†ÅÔºåÂä†ËΩΩÊåáÂÆömp4ËßÜÈ¢ëÊñá‰ª∂ÔºåÂπ∂ÂùáÂåÄÊäΩÂèñ100Â∏ßÔºå‰øùÂ≠ò‰∏∫.npzÊ†ºÂºèÁöÑnumpyÊï∞ÁªÑ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 11449, 6922, 23787, 2283, 13072, 15590, 378, 35645, 16325, 2541, 19, 6663, 7210, 378, 1068, 13351, 57015, 1570, 46870, 378, 14533, 441, 44827, 89, 136920, 63339, 33427], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59708 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:24:28 [async_llm.py:261] Added request cmpl-4bc20e2c2d11435f8576c7e510db0c67-0.
INFO 06-02 20:24:28 [ray_distributed_executor.py:561] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto
INFO 06-02 20:24:28 [ray_distributed_executor.py:563] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
INFO 06-02 20:24:28 [ray_distributed_executor.py:578] RAY_CGRAPH_get_timeout is set to 300
INFO 06-02 20:24:34 [loggers.py:116] Engine 000: Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:24:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:28:34 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO:     127.0.0.1:50402 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:43064 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
WARNING 06-02 20:30:02 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:30:02 [logger.py:42] Received request cmpl-95484a970a5443cd8559d4c3f8bd92aa-0: prompt: 'ËØ∑ÊèèËø∞ÂõæÁâá', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 6552], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48072 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:30:02 [async_llm.py:261] Added request cmpl-95484a970a5443cd8559d4c3f8bd92aa-0.
INFO 06-02 20:30:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:30:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:30:21 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:30:21 [logger.py:42] Received request cmpl-34514a08ce8b493d83617032d1c38485-0: prompt: 'ËØ∑ÊèèËø∞ÂõæÁâá', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 6552], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:30:21 [async_llm.py:261] Added request cmpl-34514a08ce8b493d83617032d1c38485-0.
INFO 06-02 20:30:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:30:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:32:57 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:32:57 [logger.py:42] Received request cmpl-c911b6a5d871470f9a82ba59c71673ab-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38378 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:32:57 [async_llm.py:261] Added request cmpl-c911b6a5d871470f9a82ba59c71673ab-0.
INFO 06-02 20:33:04 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 98.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
INFO 06-02 20:33:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:33:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:38604 - "POST /v1/completions HTTP/1.1" 400 Bad Request
WARNING 06-02 20:35:39 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:35:39 [logger.py:42] Received request cmpl-fa20b9c2983a468bb7d1dfec1149094f-0: prompt: 'Please descirbe the image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 5023, 446, 2879, 276, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:51816 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:35:39 [async_llm.py:261] Added request cmpl-fa20b9c2983a468bb7d1dfec1149094f-0.
INFO 06-02 20:35:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:35:48 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:35:48 [logger.py:42] Received request cmpl-f1331032eb1a4a78a522eaf738133084-0: prompt: 'ËØ∑ÊèèËø∞ÂõæÁâá', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 6552], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:58270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:35:48 [async_llm.py:261] Added request cmpl-f1331032eb1a4a78a522eaf738133084-0.
INFO 06-02 20:35:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:36:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:36:38 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:36:38 [logger.py:42] Received request cmpl-93220aab3504429ca9e02dfdf0086f88-0: prompt: 'ËØ∑ÊèèËø∞ÂõæÁâá', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 6552], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:36:38 [async_llm.py:261] Added request cmpl-93220aab3504429ca9e02dfdf0086f88-0.
INFO:     127.0.0.1:35194 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:36:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:36:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:37:48 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:37:48 [logger.py:42] Received request cmpl-2cc6fa77d96e441fb165c28db0c5bc78-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:37:48 [async_llm.py:261] Added request cmpl-2cc6fa77d96e441fb165c28db0c5bc78-0.
INFO:     127.0.0.1:49052 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:37:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:38:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:38:12 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:38:12 [logger.py:42] Received request cmpl-0b58ecfd3a1c4859bca3c20574805e3b-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:38:12 [async_llm.py:261] Added request cmpl-0b58ecfd3a1c4859bca3c20574805e3b-0.
INFO:     127.0.0.1:38860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:38:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-02 20:38:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
WARNING 06-02 20:39:18 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:39:18 [logger.py:42] Received request cmpl-5f3bc46094f74355b65811a6e28a6545-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41562 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:39:18 [async_llm.py:261] Added request cmpl-5f3bc46094f74355b65811a6e28a6545-0.
INFO 06-02 20:39:24 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 73.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 28.9%
INFO 06-02 20:39:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 28.9%
INFO 06-02 20:39:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 28.9%
WARNING 06-02 20:39:46 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:39:46 [logger.py:42] Received request cmpl-0c79e8786eed47a297a518eb5913903d-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:39:46 [async_llm.py:261] Added request cmpl-0c79e8786eed47a297a518eb5913903d-0.
INFO 06-02 20:39:54 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.2%
INFO 06-02 20:40:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.2%
WARNING 06-02 20:40:55 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:40:55 [logger.py:42] Received request cmpl-4389ded612c1449c8e755525a1619166-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:40:55 [async_llm.py:261] Added request cmpl-4389ded612c1449c8e755525a1619166-0.
INFO 06-02 20:40:55 [async_llm.py:420] Aborted request cmpl-4389ded612c1449c8e755525a1619166-0.
INFO 06-02 20:40:55 [async_llm.py:327] Request cmpl-4389ded612c1449c8e755525a1619166-0 aborted.
INFO 06-02 20:41:04 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.8%
INFO 06-02 20:41:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.8%
WARNING 06-02 20:44:01 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:44:01 [logger.py:42] Received request cmpl-8a49f35efc514f0cba96925c3eb10c68-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:44:01 [async_llm.py:261] Added request cmpl-8a49f35efc514f0cba96925c3eb10c68-0.
INFO 06-02 20:44:04 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 57.5%
INFO 06-02 20:44:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.5%
INFO 06-02 20:44:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.5%
WARNING 06-02 20:44:35 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:44:35 [logger.py:42] Received request cmpl-730ee3f554da4a509103780e6b39a4a2-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:44:35 [async_llm.py:261] Added request cmpl-730ee3f554da4a509103780e6b39a4a2-0.
INFO 06-02 20:44:44 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.5%
INFO 06-02 20:44:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.5%
WARNING 06-02 20:45:31 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:45:31 [logger.py:42] Received request cmpl-ca1cc8569b8543af8e89035aaefccc06-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60190 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:45:31 [async_llm.py:261] Added request cmpl-ca1cc8569b8543af8e89035aaefccc06-0.
INFO 06-02 20:45:34 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 64.6%
INFO 06-02 20:45:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.6%
INFO 06-02 20:45:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.6%
WARNING 06-02 20:46:51 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:46:51 [logger.py:42] Received request cmpl-1279c26c057e4b7b92102eec0b9425e2-0: prompt: "texts='''\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.\n'''", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=0, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3501, 82, 3286, 108177, 10489, 27548, 276, 11360, 4393, 3125, 308, 28474, 276, 4082, 316, 10586, 276, 13529, 318, 276, 4111, 7867, 25, 220, 16, 13, 119209, 8078, 347, 56, 29030, 904, 220, 17, 13, 26321, 347, 56, 29030, 1083, 17400, 276, 4626, 69951, 877, 673, 261, 53659, 11, 395, 3722, 25, 1141, 39347, 341, 61605], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36928 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:46:51 [async_llm.py:261] Added request cmpl-1279c26c057e4b7b92102eec0b9425e2-0.
INFO 06-02 20:46:54 [loggers.py:116] Engine 000: Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 39.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 66.9%
INFO 06-02 20:47:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.9%
INFO 06-02 20:47:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.9%
WARNING 06-02 20:47:20 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:47:20 [logger.py:42] Received request cmpl-75f17b7c450e4a2a8fc92d97590dbf70-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:47:20 [async_llm.py:261] Added request cmpl-75f17b7c450e4a2a8fc92d97590dbf70-0.
INFO:     127.0.0.1:42796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:47:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.4%
INFO 06-02 20:47:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.4%
WARNING 06-02 20:47:39 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:47:39 [logger.py:42] Received request cmpl-f69c74e32a1348c0b1c2880e254cb752-0: prompt: 'ËØ∑ÊèèËø∞ÂõæÁâá', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 6552], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:47:39 [async_llm.py:261] Added request cmpl-f69c74e32a1348c0b1c2880e254cb752-0.
INFO:     127.0.0.1:47490 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:47:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.0%
INFO 06-02 20:47:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.0%
INFO:     127.0.0.1:48648 - "POST /v1/completions HTTP/1.1" 400 Bad Request
WARNING 06-02 20:49:17 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:49:17 [logger.py:42] Received request cmpl-8b7e1ceb65484e1cb5af35a2eb41cef5-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá: <think>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25, 555, 39964, 29], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:49:17 [async_llm.py:261] Added request cmpl-8b7e1ceb65484e1cb5af35a2eb41cef5-0.
INFO:     127.0.0.1:46070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:49:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 40.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
INFO 06-02 20:49:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.0%
WARNING 06-02 20:49:44 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:49:44 [logger.py:42] Received request cmpl-d16aa7ed9cf54263ba140c6a0d062663-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá: <think>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25, 555, 39964, 29], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:49:44 [async_llm.py:261] Added request cmpl-d16aa7ed9cf54263ba140c6a0d062663-0.
INFO 06-02 20:49:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 64.0%
INFO:     127.0.0.1:48028 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:49:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 92.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.0%
INFO 06-02 20:50:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 64.0%
WARNING 06-02 20:50:35 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:50:35 [logger.py:42] Received request cmpl-4f07d78a217e4e309f163a35f2dc64a4-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:50:35 [async_llm.py:261] Added request cmpl-4f07d78a217e4e309f163a35f2dc64a4-0.
INFO:     127.0.0.1:39258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:50:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.4%
INFO 06-02 20:50:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.4%
WARNING 06-02 20:51:19 [protocol.py:57] The following fields were present in the request but ignored: {'image_path'}
INFO 06-02 20:51:19 [logger.py:42] Received request cmpl-26e7e95046014569b18b6c03439c96b1-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 20:51:19 [async_llm.py:261] Added request cmpl-26e7e95046014569b18b6c03439c96b1-0.
INFO 06-02 20:51:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 68.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 62.8%
INFO:     127.0.0.1:45420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 20:51:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.8%
INFO 06-02 20:51:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.8%
WARNING 06-02 21:18:27 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:18:27 [logger.py:42] Received request cmpl-5527a2f121744bd4a9dcab3ab09d0b68-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:18:27 [async_llm.py:261] Added request cmpl-5527a2f121744bd4a9dcab3ab09d0b68-0.
INFO:     127.0.0.1:41734 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:18:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.3%
INFO 06-02 21:18:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.3%
WARNING 06-02 21:18:46 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:18:46 [logger.py:42] Received request cmpl-6532e3a4b5674fecb549f91fc8a97cad-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:18:46 [async_llm.py:261] Added request cmpl-6532e3a4b5674fecb549f91fc8a97cad-0.
INFO:     127.0.0.1:48432 - "POST /v1/completions HTTP/1.1" 200 OK
WARNING 06-02 21:18:51 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:18:51 [logger.py:42] Received request cmpl-9355d7c1ac894751945f76f2bd30d9b9-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:18:51 [async_llm.py:261] Added request cmpl-9355d7c1ac894751945f76f2bd30d9b9-0.
INFO:     127.0.0.1:35930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:18:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.4%
WARNING 06-02 21:18:56 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:18:56 [logger.py:42] Received request cmpl-bb7b8b83766444619f707656d7696272-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:18:56 [async_llm.py:261] Added request cmpl-bb7b8b83766444619f707656d7696272-0.
INFO:     127.0.0.1:35940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:19:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.0%
INFO 06-02 21:19:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.0%
WARNING 06-02 21:19:53 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:19:53 [logger.py:42] Received request cmpl-af9c7731155946fc8bbb7eaf1104a039-0: prompt: 'Please describe this image', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10489, 13660, 566, 4082], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:19:53 [async_llm.py:261] Added request cmpl-af9c7731155946fc8bbb7eaf1104a039-0.
INFO:     127.0.0.1:41204 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:19:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.5%
INFO 06-02 21:20:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.5%
WARNING 06-02 21:20:35 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:20:35 [logger.py:42] Received request cmpl-969cd3f9b96942ff8741a554fe86fb41-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:20:35 [async_llm.py:261] Added request cmpl-969cd3f9b96942ff8741a554fe86fb41-0.
INFO:     127.0.0.1:59966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:20:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 41.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.0%
WARNING 06-02 21:20:45 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:20:45 [logger.py:42] Received request cmpl-87923ba6ada14eeab6083a7e1aa532e7-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:20:45 [async_llm.py:261] Added request cmpl-87923ba6ada14eeab6083a7e1aa532e7-0.
INFO:     127.0.0.1:45448 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:20:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 59.5%
INFO 06-02 21:21:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 59.5%
WARNING 06-02 21:21:04 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:21:04 [logger.py:42] Received request cmpl-f6e6eb7bce5141ac93a38b3692aaa5bd-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:21:04 [async_llm.py:261] Added request cmpl-f6e6eb7bce5141ac93a38b3692aaa5bd-0.
INFO:     127.0.0.1:52104 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:21:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 43.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.9%
INFO 06-02 21:21:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.9%
WARNING 06-02 21:21:38 [protocol.py:57] The following fields were present in the request but ignored: {'image_url'}
INFO 06-02 21:21:38 [logger.py:42] Received request cmpl-bbb599a7523e4e968386dedc11677c03-0: prompt: 'ËØ∑ÊèèËø∞ËØ•ÂõæÁâá:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2066, 12180, 1514, 6552, 25], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:21:38 [async_llm.py:261] Added request cmpl-bbb599a7523e4e968386dedc11677c03-0.
INFO 06-02 21:21:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 58.4%
INFO:     127.0.0.1:58956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-02 21:21:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.4%
INFO 06-02 21:22:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.4%
INFO:     127.0.0.1:49920 - "POST /v1/completions/chat/completions HTTP/1.1" 404 Not Found
INFO 06-02 21:26:19 [chat_utils.py:419] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
WARNING 06-02 21:26:19 [tokenizer.py:261] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 06-02 21:26:19 [logger.py:42] Received request chatcmpl-a7900be924ba4406a0b602602dd8781f: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nCan you describe the picture? Think step by step.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16349, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:26:21 [async_llm.py:261] Added request chatcmpl-a7900be924ba4406a0b602602dd8781f.
INFO 06-02 21:26:24 [loggers.py:116] Engine 000: Avg prompt throughput: 109.8 tokens/s, Avg generation throughput: 24.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 20.1%
INFO:     127.0.0.1:40984 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:26:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 56.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.1%
INFO 06-02 21:26:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.1%
INFO 06-02 21:27:28 [logger.py:42] Received request chatcmpl-ed52bd33026a46b48927ae123d818c06: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nËØ∑ÊèèËø∞ËØ•ÂõæÁâá<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16356, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:27:28 [async_llm.py:261] Added request chatcmpl-ed52bd33026a46b48927ae123d818c06.
INFO 06-02 21:27:34 [loggers.py:116] Engine 000: Avg prompt throughput: 109.1 tokens/s, Avg generation throughput: 85.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 50.9%
INFO 06-02 21:27:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 139.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 50.9%
INFO 06-02 21:27:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 50.9%
INFO 06-02 21:28:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.3%, Prefix cache hit rate: 50.9%
INFO 06-02 21:28:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.5%, Prefix cache hit rate: 50.9%
INFO 06-02 21:28:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.6%, Prefix cache hit rate: 50.9%
INFO:     127.0.0.1:56850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:28:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.9%
INFO 06-02 21:28:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.9%
INFO 06-02 21:31:05 [logger.py:42] Received request chatcmpl-9fd3f0bb7a0647c5b7bdce9716c10bdd: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nPlease describe this picture<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16356, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:31:05 [async_llm.py:261] Added request chatcmpl-9fd3f0bb7a0647c5b7bdce9716c10bdd.
INFO 06-02 21:31:05 [async_llm.py:420] Aborted request chatcmpl-9fd3f0bb7a0647c5b7bdce9716c10bdd.
INFO 06-02 21:31:05 [async_llm.py:327] Request chatcmpl-9fd3f0bb7a0647c5b7bdce9716c10bdd aborted.
INFO 06-02 21:31:52 [logger.py:42] Received request chatcmpl-4caebc45ec0e4d8583792c4a7d98926d: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nPlease describe this picture<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16356, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:31:52 [async_llm.py:261] Added request chatcmpl-4caebc45ec0e4d8583792c4a7d98926d.
INFO 06-02 21:31:54 [loggers.py:116] Engine 000: Avg prompt throughput: 109.1 tokens/s, Avg generation throughput: 35.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 72.1%
INFO:     127.0.0.1:51764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:32:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 72.1%
INFO 06-02 21:32:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 72.1%
INFO 06-02 21:33:12 [logger.py:42] Received request chatcmpl-76e2ae569b00488c9103a6a07ca82dca: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nËØ∑ÊèèËø∞ËØ•ÂõæÁâá<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16356, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:33:12 [async_llm.py:261] Added request chatcmpl-76e2ae569b00488c9103a6a07ca82dca.
INFO 06-02 21:33:14 [loggers.py:116] Engine 000: Avg prompt throughput: 109.1 tokens/s, Avg generation throughput: 28.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 77.1%
INFO:     127.0.0.1:50912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:33:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 77.1%
INFO 06-02 21:33:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 77.1%
INFO 06-02 21:34:29 [logger.py:42] Received request chatcmpl-9ee8f444fb0f4154886b2b53347555c0: prompt: '<|im_start|>user\n<|media_start|>image<|media_content|><|media_pad|><|media_end|>\nPlease describe this picture<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16356, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-02 21:34:29 [async_llm.py:261] Added request chatcmpl-9ee8f444fb0f4154886b2b53347555c0.
INFO 06-02 21:34:34 [loggers.py:116] Engine 000: Avg prompt throughput: 109.1 tokens/s, Avg generation throughput: 60.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 80.6%
INFO 06-02 21:34:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 131.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 80.6%
INFO 06-02 21:34:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 108.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.0%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.0%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 100.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.1%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 94.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 80.6%
INFO 06-02 21:35:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.2%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.1%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.0%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.9%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.7%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:44 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.5%, Prefix cache hit rate: 80.6%
INFO 06-02 21:36:54 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.3%, Prefix cache hit rate: 80.6%
INFO 06-02 21:37:04 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 80.6%
INFO 06-02 21:37:14 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.8%, Prefix cache hit rate: 80.6%
INFO:     127.0.0.1:39348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-02 21:37:24 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.6%
INFO 06-02 21:37:34 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.6%
