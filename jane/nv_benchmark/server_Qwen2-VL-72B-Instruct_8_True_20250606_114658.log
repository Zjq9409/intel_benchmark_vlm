INFO 06-06 11:47:14 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:47:30 [api_server.py:1043] vLLM API server version 0.8.5.post1
INFO 06-06 11:47:30 [api_server.py:1044] args: Namespace(host='127.0.0.1', port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='../../template_chatml.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/disk0/LLM/Qwen2-VL-72B-Instruct/', task='auto', tokenizer='/disk0/LLM/Qwen2-VL-72B-Instruct/', hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='bfloat16', max_model_len=16384, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=8, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=256, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
INFO 06-06 11:48:24 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 06-06 11:48:51 [config.py:1771] Defaulting to use mp for distributed inference
INFO 06-06 11:48:51 [config.py:2004] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 06-06 11:48:53 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:55 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/disk0/LLM/Qwen2-VL-72B-Instruct/', speculative_config=None, tokenizer='/disk0/LLM/Qwen2-VL-72B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/disk0/LLM/Qwen2-VL-72B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-06 11:48:55 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-06 11:48:55 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 10485760, 10, 'psm_a9793e0c'), local_subscribe_addr='ipc:///tmp/d5761573-b31a-4f56-9e4b-9758f6db072d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
INFO 06-06 11:48:57 [__init__.py:239] Automatically detected platform cuda.
WARNING 06-06 11:48:59 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x75602e7ccca0>
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:48:59 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2cf7b175'), local_subscribe_addr='ipc:///tmp/e3c38bec-4e22-4e85-aafc-44bdbbb1cd65', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-06 11:48:59 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7254cc8dcac0>
WARNING 06-06 11:48:59 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x710fc23bcc40>
WARNING 06-06 11:48:59 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x79de38ca8b80>
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:48:59 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2a265438'), local_subscribe_addr='ipc:///tmp/75c79815-e1c1-4540-b377-d4dee549119e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:48:59 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3def11bb'), local_subscribe_addr='ipc:///tmp/6dd04991-25ed-44a5-b965-7878aeeb2d86', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:48:59 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b866711d'), local_subscribe_addr='ipc:///tmp/609bb245-5992-4590-bff0-caf82d3bd62f', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-06 11:49:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc89b3f4c70>
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:00 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_61bcde95'), local_subscribe_addr='ipc:///tmp/f16987d9-c199-4214-8830-fb9d7c35eae0', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-06 11:49:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b07c949ca90>
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:00 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8a51fe2d'), local_subscribe_addr='ipc:///tmp/590c88a0-d7e7-404d-bccf-c7459a46a515', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-06 11:49:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7258dc7ecc40>
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:00 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a9dce45'), local_subscribe_addr='ipc:///tmp/9404d441-27eb-4405-81af-d29f5848b92b', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-06 11:49:00 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe0132f4bb0>
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:00 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4ccb9b65'), local_subscribe_addr='ipc:///tmp/a4b98ad4-cd32-47d3-b09e-36e7a88797ab', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:02 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:02 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m WARNING 06-06 11:49:02 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_26b15032'), local_subscribe_addr='ipc:///tmp/e2a0bb87-c8d1-411d-9411-3301e02e86d6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:02 [parallel_state.py:1004] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:02 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m WARNING 06-06 11:49:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:06 [gpu_model_runner.py:1329] Starting to load model /disk0/LLM/Qwen2-VL-72B-Instruct/...
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m WARNING 06-06 11:49:06 [vision.py:93] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:06 [config.py:3615] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512] is overridden by config [512, 384, 256, 128, 4, 2, 1, 392, 264, 136, 8, 400, 272, 144, 16, 408, 280, 152, 24, 416, 288, 160, 32, 424, 296, 168, 40, 432, 304, 176, 48, 440, 312, 184, 56, 448, 320, 192, 64, 456, 328, 200, 72, 464, 336, 208, 80, 472, 344, 216, 88, 120, 480, 352, 248, 224, 96, 488, 504, 360, 232, 104, 496, 368, 240, 112, 376]
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:12 [loader.py:458] Loading weights took 6.33 seconds
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:12 [loader.py:458] Loading weights took 6.44 seconds
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.645328 seconds
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.75 seconds
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.563457 seconds
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.969407 seconds
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.65 seconds
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.59 seconds
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.64 seconds
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.68 seconds
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.808165 seconds
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.850642 seconds
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.863964 seconds
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:13 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 6.902719 seconds
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:13 [loader.py:458] Loading weights took 6.89 seconds
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:14 [gpu_model_runner.py:1347] Model loading took 17.1900 GiB and 7.122810 seconds
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:16 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_4_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_7_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.68 s
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.68 s
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_5_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.70 s
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.76 s
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.79 s
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_6_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.82 s
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.83 s
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:32 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/1765e21639/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:32 [backends.py:430] Dynamo bytecode transform time: 11.88 s
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.453 s
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.502 s
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.504 s
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.538 s
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.533 s
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.533 s
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:40 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.583 s
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:41 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 7.609 s
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.79 s in total
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.68 s in total
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.70 s in total
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.76 s in total
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.83 s in total
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.88 s in total
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.82 s in total
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:49:42 [monitor.py:33] torch.compile takes 11.68 s in total
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
INFO 06-06 11:49:43 [kv_cache_utils.py:634] GPU KV cache size: 25,088 tokens
INFO 06-06 11:49:43 [kv_cache_utils.py:637] Maximum concurrency for 16,384 tokens per request: 1.53x
[1;36m(VllmWorker rank=4 pid=1053145)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
[1;36m(VllmWorker rank=5 pid=1053146)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
[1;36m(VllmWorker rank=6 pid=1053147)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
[1;36m(VllmWorker rank=3 pid=1053144)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.64 GiB
[1;36m(VllmWorker rank=7 pid=1053148)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
[1;36m(VllmWorker rank=0 pid=1053141)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
[1;36m(VllmWorker rank=2 pid=1053143)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.64 GiB
[1;36m(VllmWorker rank=1 pid=1053142)[0;0m INFO 06-06 11:50:23 [gpu_model_runner.py:1686] Graph capturing finished in 40 secs, took 2.63 GiB
INFO 06-06 11:50:24 [core.py:159] init engine (profile, create kv cache, warmup model) took 70.07 seconds
INFO 06-06 11:50:27 [core_client.py:439] Core engine process 0 ready.
WARNING 06-06 11:50:28 [api_server.py:945] Using supplied chat template: {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n'}}{% endif %}{% endfor %}
WARNING 06-06 11:50:28 [api_server.py:945] {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n' }}{% endif %}
WARNING 06-06 11:50:28 [api_server.py:945] It is different from official chat template '/disk0/LLM/Qwen2-VL-72B-Instruct/'. This discrepancy may lead to performance degradation.
WARNING 06-06 11:50:28 [config.py:1240] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-06 11:50:28 [serving_chat.py:118] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'top_k': 1, 'top_p': 0.001}
INFO 06-06 11:50:28 [serving_completion.py:61] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'top_k': 1, 'top_p': 0.001}
INFO 06-06 11:50:28 [api_server.py:1090] Starting vLLM API server on http://127.0.0.1:8000
INFO 06-06 11:50:28 [launcher.py:28] Available routes are:
INFO 06-06 11:50:28 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /health, Methods: GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /load, Methods: GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /version, Methods: GET
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /score, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-06 11:50:28 [launcher.py:36] Route: /metrics, Methods: GET
INFO 06-06 11:56:45 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 06-06 11:56:45 [logger.py:39] Received request chatcmpl-2db41a60f62948e3b44db43015432f05: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nEmily celebrated her birthday on Thursday, and her sister Liepa 8 days earlier. Which weekday was that? Option:[ \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Tuesday\\", \\"Sunday\\" ]\r \r<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:38284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:56:46 [async_llm.py:252] Added request chatcmpl-2db41a60f62948e3b44db43015432f05.
INFO 06-06 11:56:48 [loggers.py:111] Engine 000: Avg prompt throughput: 20.4 tokens/s, Avg generation throughput: 4.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
INFO 06-06 11:56:50 [logger.py:39] Received request chatcmpl-c000b69cddba4ca6808d47ada1278db8: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nEmily celebrated her birthday on Thursday, and her sister Liepa 8 days earlier. Which weekday was that? Option:[ \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Tuesday\\", \\"Sunday\\" ]\r \r<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:56:50 [async_llm.py:252] Added request chatcmpl-c000b69cddba4ca6808d47ada1278db8.
INFO 06-06 11:56:55 [logger.py:39] Received request chatcmpl-dd90b687258a448f89a19b6d7a116185: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nWhich number do you have to write in the last daisy?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:56:55 [async_llm.py:252] Added request chatcmpl-dd90b687258a448f89a19b6d7a116185.
INFO 06-06 11:56:58 [loggers.py:111] Engine 000: Avg prompt throughput: 69.8 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 22.2%
INFO 06-06 11:56:59 [logger.py:39] Received request chatcmpl-56440c764ede44f7bc7cda937e5b68e7: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nWhich number do you have to write in the last daisy?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:56:59 [async_llm.py:252] Added request chatcmpl-56440c764ede44f7bc7cda937e5b68e7.
INFO 06-06 11:57:04 [logger.py:39] Received request chatcmpl-543383bf46614683a2ac654e27fd82a6: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nHow many ropes can you see in this picture?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:04 [async_llm.py:252] Added request chatcmpl-543383bf46614683a2ac654e27fd82a6.
INFO 06-06 11:57:05 [logger.py:39] Received request chatcmpl-bffee8c0902548558b5239075d822177: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nHow many ropes can you see in this picture?<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:05 [async_llm.py:252] Added request chatcmpl-bffee8c0902548558b5239075d822177.
INFO 06-06 11:57:08 [logger.py:39] Received request chatcmpl-6f788aaf181d4d6eb3d04c941bcdf683: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:08 [async_llm.py:252] Added request chatcmpl-6f788aaf181d4d6eb3d04c941bcdf683.
INFO 06-06 11:57:08 [loggers.py:111] Engine 000: Avg prompt throughput: 84.4 tokens/s, Avg generation throughput: 21.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 11:57:18 [loggers.py:111] Engine 000: Avg prompt throughput: 121.9 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.3%, Prefix cache hit rate: 28.9%
INFO 06-06 11:57:22 [logger.py:39] Received request chatcmpl-d9d6d5e820a34eb18b760d67555f25a3: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:22 [async_llm.py:252] Added request chatcmpl-d9d6d5e820a34eb18b760d67555f25a3.
INFO 06-06 11:57:28 [loggers.py:111] Engine 000: Avg prompt throughput: 121.9 tokens/s, Avg generation throughput: 36.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 50.0%
INFO 06-06 11:57:38 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 11:57:38 [logger.py:39] Received request chatcmpl-c1fb7d92fb7a4682ae0cb8394fa78c01: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:38 [async_llm.py:252] Added request chatcmpl-c1fb7d92fb7a4682ae0cb8394fa78c01.
INFO 06-06 11:57:48 [loggers.py:111] Engine 000: Avg prompt throughput: 57.7 tokens/s, Avg generation throughput: 35.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 43.8%
INFO 06-06 11:57:52 [logger.py:39] Received request chatcmpl-e251c21ae8674ce2944fba12716ef248: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:57:52 [async_llm.py:252] Added request chatcmpl-e251c21ae8674ce2944fba12716ef248.
INFO 06-06 11:57:58 [loggers.py:111] Engine 000: Avg prompt throughput: 57.7 tokens/s, Avg generation throughput: 37.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 50.0%
INFO 06-06 11:58:06 [logger.py:39] Received request chatcmpl-212701b5b9be42a092057f5d1cc95832: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:58:06 [async_llm.py:252] Added request chatcmpl-212701b5b9be42a092057f5d1cc95832.
INFO 06-06 11:58:08 [loggers.py:111] Engine 000: Avg prompt throughput: 72.5 tokens/s, Avg generation throughput: 27.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 44.0%
INFO 06-06 11:58:16 [logger.py:39] Received request chatcmpl-eca7848ee65444dd935bba2aa83fc9c6: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n目标代码详见图上半部分 snapshot-code，测试报错信息详见图下半部分 snapshot-feedback. 请尝试定位代码遇到的问题，并给出分析，如果可能，给出解决方案.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:58:16 [async_llm.py:252] Added request chatcmpl-eca7848ee65444dd935bba2aa83fc9c6.
INFO 06-06 11:58:18 [loggers.py:111] Engine 000: Avg prompt throughput: 72.5 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 50.0%
INFO 06-06 11:58:28 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 32.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 11:58:29 [logger.py:39] Received request chatcmpl-124fcaed28e34f878d8dfa1068253d66: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:32882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:58:29 [async_llm.py:252] Added request chatcmpl-124fcaed28e34f878d8dfa1068253d66.
INFO 06-06 11:58:38 [loggers.py:111] Engine 000: Avg prompt throughput: 75.1 tokens/s, Avg generation throughput: 33.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 45.0%
INFO 06-06 11:58:42 [logger.py:39] Received request chatcmpl-25add2b3c9d440aea67775b22e7f124d: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:58:42 [async_llm.py:252] Added request chatcmpl-25add2b3c9d440aea67775b22e7f124d.
INFO 06-06 11:58:48 [loggers.py:111] Engine 000: Avg prompt throughput: 75.1 tokens/s, Avg generation throughput: 36.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 50.0%
INFO 06-06 11:58:56 [logger.py:39] Received request chatcmpl-151eb6b750ea4df0bb3a4ec01f1f18c7: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:58:56 [async_llm.py:252] Added request chatcmpl-151eb6b750ea4df0bb3a4ec01f1f18c7.
INFO 06-06 11:58:58 [loggers.py:111] Engine 000: Avg prompt throughput: 146.8 tokens/s, Avg generation throughput: 26.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 42.4%
INFO 06-06 11:59:07 [logger.py:39] Received request chatcmpl-beaf820c53574faba11eab2fe5b53afe: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:59:07 [async_llm.py:252] Added request chatcmpl-beaf820c53574faba11eab2fe5b53afe.
INFO 06-06 11:59:08 [loggers.py:111] Engine 000: Avg prompt throughput: 146.8 tokens/s, Avg generation throughput: 36.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 50.0%
INFO 06-06 11:59:18 [logger.py:39] Received request chatcmpl-94d88a711b7d4428bfa51ca577988650: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:59:18 [async_llm.py:252] Added request chatcmpl-94d88a711b7d4428bfa51ca577988650.
INFO 06-06 11:59:18 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 28.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 11:59:28 [loggers.py:111] Engine 000: Avg prompt throughput: 526.2 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.1%, Prefix cache hit rate: 33.9%
INFO 06-06 11:59:31 [logger.py:39] Received request chatcmpl-7e7221a4d25e42c1b7baeca810cbc227: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:59:31 [async_llm.py:252] Added request chatcmpl-7e7221a4d25e42c1b7baeca810cbc227.
INFO 06-06 11:59:38 [loggers.py:111] Engine 000: Avg prompt throughput: 526.2 tokens/s, Avg generation throughput: 32.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.9%, Prefix cache hit rate: 50.0%
INFO 06-06 11:59:43 [logger.py:39] Received request chatcmpl-de24df4f7aa543f68642d11247a06a1b: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52202 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:59:43 [async_llm.py:252] Added request chatcmpl-de24df4f7aa543f68642d11247a06a1b.
INFO 06-06 11:59:48 [loggers.py:111] Engine 000: Avg prompt throughput: 126.3 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 47.3%
INFO 06-06 11:59:57 [logger.py:39] Received request chatcmpl-cd6b662b8bfd425383ce127cc40794f1: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 11:59:57 [async_llm.py:252] Added request chatcmpl-cd6b662b8bfd425383ce127cc40794f1.
INFO 06-06 11:59:58 [loggers.py:111] Engine 000: Avg prompt throughput: 126.3 tokens/s, Avg generation throughput: 36.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 50.0%
INFO 06-06 12:00:08 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.7%, Prefix cache hit rate: 50.0%
INFO 06-06 12:00:13 [logger.py:39] Received request chatcmpl-e1174a1c10af413e9bd6bb64db799ea1: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:00:13 [async_llm.py:252] Added request chatcmpl-e1174a1c10af413e9bd6bb64db799ea1.
INFO 06-06 12:00:18 [loggers.py:111] Engine 000: Avg prompt throughput: 235.6 tokens/s, Avg generation throughput: 23.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.9%, Prefix cache hit rate: 45.5%
INFO 06-06 12:00:28 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 45.5%
INFO 06-06 12:00:28 [logger.py:39] Received request chatcmpl-5e61100e6181424bb3fba347f6dd54dc: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:40306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:00:28 [async_llm.py:252] Added request chatcmpl-5e61100e6181424bb3fba347f6dd54dc.
INFO 06-06 12:00:38 [loggers.py:111] Engine 000: Avg prompt throughput: 235.6 tokens/s, Avg generation throughput: 36.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.8%, Prefix cache hit rate: 50.0%
INFO 06-06 12:00:45 [logger.py:39] Received request chatcmpl-619e9e6d9fa64954bdaf8d10c7b0eb9a: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:00:45 [async_llm.py:252] Added request chatcmpl-619e9e6d9fa64954bdaf8d10c7b0eb9a.
INFO 06-06 12:00:48 [loggers.py:111] Engine 000: Avg prompt throughput: 343.3 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.0%, Prefix cache hit rate: 44.7%
INFO 06-06 12:00:58 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 36.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 44.7%
INFO 06-06 12:01:01 [logger.py:39] Received request chatcmpl-85c967e44aea4ab4863547297c825426: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\n请详细描述图像内容<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:33002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:01 [async_llm.py:252] Added request chatcmpl-85c967e44aea4ab4863547297c825426.
INFO 06-06 12:01:08 [loggers.py:111] Engine 000: Avg prompt throughput: 343.3 tokens/s, Avg generation throughput: 35.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 50.0%
INFO 06-06 12:01:17 [logger.py:39] Received request chatcmpl-b9c0639971854ab7b61da456eb85fb98: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:17 [async_llm.py:252] Added request chatcmpl-b9c0639971854ab7b61da456eb85fb98.
INFO 06-06 12:01:18 [logger.py:39] Received request chatcmpl-a52fecf8dd734565924d608e46e495b3: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:53380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:18 [async_llm.py:252] Added request chatcmpl-a52fecf8dd734565924d608e46e495b3.
INFO 06-06 12:01:18 [loggers.py:111] Engine 000: Avg prompt throughput: 212.0 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 12:01:20 [logger.py:39] Received request chatcmpl-9f4fa8c7e4514dc8abfbd6bfae1b05eb: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:20 [async_llm.py:252] Added request chatcmpl-9f4fa8c7e4514dc8abfbd6bfae1b05eb.
INFO 06-06 12:01:21 [logger.py:39] Received request chatcmpl-fd76256c8cb741d1b804829d0fb00f47: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:21 [async_llm.py:252] Added request chatcmpl-fd76256c8cb741d1b804829d0fb00f47.
INFO 06-06 12:01:23 [logger.py:39] Received request chatcmpl-fef0caefd88e49cf9b74189bee24624d: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:23 [async_llm.py:252] Added request chatcmpl-fef0caefd88e49cf9b74189bee24624d.
INFO 06-06 12:01:25 [logger.py:39] Received request chatcmpl-a96568468858457193255b7d66888b43: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:25 [async_llm.py:252] Added request chatcmpl-a96568468858457193255b7d66888b43.
INFO 06-06 12:01:27 [logger.py:39] Received request chatcmpl-6aa272bdf9604254af5342f92aae875d: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:27 [async_llm.py:252] Added request chatcmpl-6aa272bdf9604254af5342f92aae875d.
INFO 06-06 12:01:28 [loggers.py:111] Engine 000: Avg prompt throughput: 819.9 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 12:01:29 [logger.py:39] Received request chatcmpl-6b62cfa4e9ae49d29dbf9707d6305c9f: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nPlease utilize the visual large model to analyze the image and determine the presence of the following elements: 1. Entrance door (Y/N), 2. Package (Y/N). Output the results concatenated by a comma, for example: Y,N.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 12:01:29 [async_llm.py:252] Added request chatcmpl-6b62cfa4e9ae49d29dbf9707d6305c9f.
INFO 06-06 12:01:38 [loggers.py:111] Engine 000: Avg prompt throughput: 549.8 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 12:01:48 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 13:39:15 [logger.py:39] Received request chatcmpl-f5157b7853864d21abe046d69c55c924: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nbanish this chatbot to hell for liking pineapple pizza<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 13:39:15 [async_llm.py:252] Added request chatcmpl-f5157b7853864d21abe046d69c55c924.
INFO 06-06 13:39:18 [loggers.py:111] Engine 000: Avg prompt throughput: 87.3 tokens/s, Avg generation throughput: 11.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 49.2%
INFO 06-06 13:39:18 [logger.py:39] Received request chatcmpl-84e29024a00548f1b5026ab4b7a1be0a: prompt: '<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nbanish this chatbot to hell for liking pineapple pizza<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:43046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-06 13:39:18 [async_llm.py:252] Added request chatcmpl-84e29024a00548f1b5026ab4b7a1be0a.
INFO 06-06 13:39:28 [loggers.py:111] Engine 000: Avg prompt throughput: 87.3 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
INFO 06-06 13:39:38 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 50.0%
